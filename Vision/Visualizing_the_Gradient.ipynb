{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901aad0c",
   "metadata": {},
   "source": [
    "### Visualizing the Gradient\n",
    "https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Visualizing-the-Gradient\n",
    "\n",
    "The following code generates a surface plot of the function $f(x,y) = 2x^2 + xy$ and a quiver plot of ∇⃗ f(x, y) = [4x + yx]. Run the code in a Jupyter Notebook to see the plot. Change the line `Z = 2 * X ** 2 + X * Y` to plot some of the other functions we have discussed; note that you should use MyGrad functions (e.g. `mg.exp` and `mg.cos`) if needed.\n",
    "\n",
    "Imagine that you are standing at some point on the surface; notice that the derivative of f, or the slope of the graph where you’re standing, is different depending on the direction you are facing. For example, at the point (0,0), if you are facing the +x direction the slope is nearly flat, whereas facing the +y direction there is a very large slope. The gradient of f, or ∇⃗ f, points in the direction of steepest ascent for the point it is evaluated at.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3aa597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mygrad as mg\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "_range = (-5, 5, 0.05)\n",
    "X, Y = mg.arange(*_range), mg.arange(*_range).reshape(-1, 1)\n",
    "\n",
    "###\n",
    "Z = 2 * X ** 2 + X * Y\n",
    "###\n",
    "\n",
    "# compute the ∂Z/∂X and ∂Z/∂Y at the\n",
    "# sampled points in X and Y\n",
    "mg.sum(Z).backward()\n",
    "\n",
    "_cmap = plt.get_cmap(\"GnBu\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "ax1.set_title(\"Surface Plot of $f(x,y)=2x^2+xy$\")\n",
    "\n",
    "# get underlying numpy arrays for plotting\n",
    "U = X.grad[::20]\n",
    "V = Y.grad[::20]\n",
    "X = X.data\n",
    "Y = Y.data\n",
    "Z = Z.data\n",
    "zeros = np.full_like(Z[::20, ::20], Z.min() - 1e-2)\n",
    "\n",
    "# reduce the sampling for the quiver plot so that arrows are distinuishable\n",
    "ax1.quiver(X[::20], Y[::20], zeros, U, V, zeros, length=0.4, normalize=True)\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=_cmap, alpha=0.75)\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7d66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9b22a0",
   "metadata": {},
   "source": [
    "### Minimizing a Function Using Gradient Descent\n",
    "https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Minimizing-a-Function-Using-Gradient-Descent\n",
    "\n",
    "Write a program that performs gradient descent on the function $ f(x) = \\frac{1}{2}x^2 + e^{−x}$. Your program should take a starting coordinate `x` and a number of iterations `n`. Try running your algorithm for a few hundred iterations to see if you end up near the minimum around `x = 0.56`. Experiment with δ to see if there is a value that is small enough to avoid overshooting the minimum but large enough to efficiently narrow in on it (avoiding an excessive number of iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608bbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gradient descent on our function given\n",
    "# a starting value of x_start for n iterations\n",
    "\n",
    "def grad_descent(x_start, n):\n",
    "    # defining the gradient of our function\n",
    "    def grad(x):\n",
    "        return x - np.exp(-x) # df/dx @ the values in `x`\n",
    "\n",
    "    delta = 0.1 # step size; experiment with this value\n",
    "    x_old = x_start\n",
    "    for _ in range(n):\n",
    "        x_new = x_old - delta * grad(x_old)\n",
    "        x_old = x_new\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8904f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent(-1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent(-10, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e77424",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent(20, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68611ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a50bc38",
   "metadata": {},
   "source": [
    "### Autodifferentiation with Multivariable Functions\n",
    "https://rsokl.github.io/CogWeb/Math_Materials/Multivariable_Calculus.html#Autodifferentiation-with-Multivariable-Functions\n",
    "\n",
    "Autodifferentiation libraries, like MyGrad, can be used to compute the partial derivatives of multivariable functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06209eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using mygrad to compute the derivatives of a multivariable function\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "x = mg.tensor(3)\n",
    "y = mg.tensor(4)\n",
    "\n",
    "f = 2 * (x ** 2) + x * y\n",
    "f.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c85d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores ∂f/∂x @ x=3, y=4\n",
    "x.grad\n",
    "\n",
    "#array(16.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b800be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores ∂f/∂y @ x=3, y=4\n",
    "y.grad\n",
    "\n",
    "#array(3.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5c41b",
   "metadata": {},
   "source": [
    "### Programming Multivariable Gradient Descent\n",
    "\n",
    "Write a program that performs gradient descent on the function $f(x,y) = 2x^2 + 4y^2 + e^{−3x}  + 3e^{−2y}$. Your program should take starting coordinates `x` and `y` and a number of iterations `n`. Try running your algorithm for a few hundred iterations to see if you end up near the minimum around `x=0.3026, y=0.3629`. Experiment with δ to see if there is a value that is small enough to avoid overshooting the minimum but large enough to efficiently narrow in on it (avoiding an excessive number of iterations).\n",
    "\n",
    "Warning: Make sure that when you are updating the value of `Tensor`s, you perform the update to `Tensor.data` and not to the `Tensor` itself, to avoid back-propagating through the operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gradient descent on our function\n",
    "# given starting values of x_start and y_start for n iterations\n",
    "def multi_grad_descent(x_start, y_start, n):\n",
    "\n",
    "    # convert x and y to Tensors so that\n",
    "    # we can compute their partial derivatives\n",
    "    x = mg.tensor(x, dtype=np.float64)\n",
    "    y = mg.tensor(y, dtype=np.float64)\n",
    "\n",
    "    # defining our function; we use MyGrad operations\n",
    "    # instead of NumPy so that we can compute derivatives\n",
    "    # through these functions\n",
    "    def f(x, y):\n",
    "        return 2 * (x ** 2) + 4 * (y ** 2) + mg.exp(-3 * x) + 3 * mg.exp(-2 * y)\n",
    "\n",
    "\n",
    "    # step size; experiment with this value\n",
    "    delta = 0.1\n",
    "\n",
    "    for _ in range(n):\n",
    "        # calculating the gradient and updating the parameters\n",
    "        z = f(x, y)\n",
    "        z.backward()\n",
    "        x.data -= delta * x.grad # x.grad stores ∂f/∂x @ current x and y value\n",
    "        y.data -= delta * y.grad # y.grad stores ∂f/∂x @ current x and y value\n",
    "\n",
    "    return x.item(), y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_grad_descent(14, -53, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f286975",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_grad_descent(14, -53, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_grad_descent(14, -53, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
