{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f35b76e",
   "metadata": {},
   "source": [
    "\n",
    "# Image Search Tutorial\n",
    "\n",
    "In this notebook we will use the `image_searcher` code base to train an a linear embedding matrix that maps shape-$(512,)$ ResNet descriptor vectors of COCO images into a word-embedding space.\n",
    "This will enable us to query images based on user-submitted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f20da",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> searcher(\"horses on a beach\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785cdf",
   "metadata": {},
   "source": [
    "![Horses on a beach](https://user-images.githubusercontent.com/29104956/126541255-1e1353a3-cd38-4e00-a0e9-baf9370a9eb6.png)\n",
    "\n",
    "\n",
    "Image-descriptor vectors (which have already been processed for us) will be denoted by $\\vec{d}$.\n",
    "Vectors in the embedded language space will be denoted by $\\vec{w}$.\n",
    "\n",
    "That is, we want to learn the following linear encoding:\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} W_{\\mathrm{embed}} = \\begin{bmatrix}\\leftarrow & \\vec{w}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where $\\vec{d}_{\\mathrm{image}}$ is a $512$-dimensional descriptor vector of an image, produced by a pre-trained ResNet-18 image-classification model.\n",
    "$\\vec{w}_{\\mathrm{image}}$ is a $D$-dimensional embedding of this image descriptor.\n",
    "We want this embedded vector to \"live\" in the same \"semantic space\" as word embeddings.\n",
    "\n",
    "Suppose we want to search for pictures of \"horses on a beach\".\n",
    "We can use the $D$-dimensional GloVe embedding for each word in this \"caption\", and sum these word-embeddings with weights determined by the inverse document frequency (IDF) of each word (we will discuss how these IDFs get computed later).\n",
    "Thus we can form the embedding for this caption as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} = \\vec{w}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{w}_{\\mathrm{horses}}$ is the $D$-dimensional GloVe embedding vector for the word \"horses\", and $\\mathrm{IDF(\\mathrm{horses}})$ is the inverse document-frequency for \"horses\" (a positive scalar quantity).\n",
    "\n",
    "If we have a picture depicting horses on a beach and its corresponding descriptor vector, $\\vec{d}_{\\mathrm{image}}$ (which we are given â€“ these image descriptor vectors have been pre-created for us), then we want to be able to embed the descriptor vector for that image such that an embedding vector for the caption, $\\vec{w}_{\\mathrm{caption}}$ overlaps substantially with the image's embedding, $\\vec{w}_{\\mathrm{image}}$.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{d}_{\\mathrm{\\mathrm{image}}}W_{\\mathrm{embed}} \\rightarrow \\vec{w}_{\\mathrm{image}}\\\\\n",
    "\\hat{w}_{\\mathrm{image}}\\cdot \\hat{w}_{\\mathrm{caption}} >> 0\\\\\n",
    "\\end{equation}\n",
    "\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from image_searcher.coco import CocoData\n",
    "from image_searcher.utils import download_image\n",
    "from image_searcher.text import get_words, inverse_document_frequency, _compute_doc_freq\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0a9fc",
   "metadata": {},
   "source": [
    "<!-- #region -->\n",
    "## Our Data\n",
    "\n",
    "Please check out the [README](https://github.com/CogWorksBWSI/LanguageProject_MyGrad) before proceeding with this section.\n",
    "\n",
    "We are going to be working with the [MSCOCO 2014 dataset](https://cocodataset.org/#home).\n",
    "This dataset consists of 82,783 images, and each image has at least five plain-text captions that describe that image.\n",
    "These images have also been processed using a pre-trained ResNet-18 classification model, such that we also have a $512$-dimensional descriptor vector, $\\vec{d}_{\\mathrm{\\mathrm{image}}}$, associated with each image, which captures the contents of that image in an abstract way.\n",
    "\n",
    "All of the pertinent data for this project is found in three data file:\n",
    "1. Images and associated captions from the MSCOCO 2014 dataset. All of this information is stored in the `data/captions_train2014.json` JSON file. A few notes about this:\n",
    "    - We won't download all of the images at once, rather we will have a URL that we can use to download any given image.\n",
    "    - Each image has associated with it at least one, but possibly more, plain-text captions that describe it.\n",
    "2. A shape-$(1, 512)$ descriptor vector, $\\vec{d}_{\\mathrm{\\mathrm{image}}}$,  for each image from the MSCOCO dataset. Each of these was produced by processing each image with a pre-trained ResNet-18 classification model. This serves as an enriched/abstract encoding for each image in the dataset. `data/resnet18_features.pkl` contains a dictionary of `image-ID -> descriptor-vector` mappings, where `image-ID` is a unique integer ID for each image in the COCO-dataset.\n",
    "There are three files that we need for this project:\n",
    "3. The GloVe-200 word embeddings for a broad vocabulary of words. This will be used to compute $D=200$-dimensional embedding vectors $\\hat{w}_{\\mathrm{caption}}$ for each caption. These are stored in `\"data/glove.6B.200d.txt.w2v\"`\n",
    "\n",
    "\n",
    "### Loading COCO Data\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "# load COCO metadata\n",
    "filename = \"data/captions_train2014.json\"\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "coco = CocoData(coco_data)\n",
    "\n",
    "assert coco.caption_ids == tuple(sorted(a[\"id\"] for a in coco_data[\"annotations\"]))\n",
    "assert coco.image_ids == tuple(sorted(a[\"id\"] for a in coco_data[\"images\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886a07d",
   "metadata": {},
   "source": [
    "The `\"data/captions_train2014.json\"` JSON file has two fields that we care about: \"images\" and \"annotations\".\n",
    "\n",
    "`coco_data[\"images\"]` contains a list; each entry corresponds to a distinct **image**. For example `image_info = coco_data[\"images\"][0]` stores information for the first image.\n",
    "Each such entry contains:\n",
    "- A unique integer ID for the image (`image_info[\"id\"]`)\n",
    "- The URL where you can download the image (`image_info[\"coco_url\"]`)\n",
    "- The shape of the image (`image_info[\"height\"]`, `image_info[\"width\"]`)\n",
    "\n",
    "`coco_data[\"annotations\"]` contains a list; each entry corresponds to a distinct **caption**. For example `caption_info = coco_data[\"annotations\"][0]` stores information for the first caption.\n",
    "Each such entry contains:\n",
    "- A unique integer ID for the caption (`caption_info[\"id\"]`)\n",
    "- The ID of the image that this caption is associated with (`caption_info[\"image_id\"]`)\n",
    "- The caption, stored as a string (`caption_info[\"caption\"]`)\n",
    "\n",
    "Keep in mind that there are multiple captions associated with each image. Thus there are 82,783 entries to `coco_data[\"images\"]` and 414,113 entries to `coco_data[\"annotations\"]`.\n",
    "\n",
    "\n",
    "#### Organizing this data: the CocoData class\n",
    "\n",
    "Note that students are not provided the `CocoData`, this is part of the \"solution\" implementation.\n",
    "Students should develop functionality that is akin to this.\n",
    "\n",
    "`image_searcher.coco.CocoData` is a class that helps us to organize all of the image and caption associations, as well as create embedding vectors for each caption.\n",
    "This class stores:\n",
    "- All the image IDs\n",
    "- All the caption IDs\n",
    "- Various mappings between image/caption IDs, and associating caption-IDs with captions\n",
    "   - `image-ID -> [cap-ID-1, cap-ID-2, ...]`\n",
    "   - `caption-ID -> image-ID`\n",
    "   - `caption-ID -> \"two dogs on the grass\"`\n",
    "\n",
    "\n",
    "### Loading GloVe Embedding and Creating Embeddings of Our Captions\n",
    "First, we will load the GloVe-200 embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"data/glove.6B.200d.txt.w2v\"\n",
    "glove = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404633b",
   "metadata": {},
   "source": [
    "<!-- #region -->\n",
    "Note that we can query `glove` like a dictionary to get the shape-(D=200,) embedding for any word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083340d",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> glove[\"cat\"]\n",
    "array([ 0.14823  , -0.53152  , -0.25973  , -0.44095  ,  0.38555  ,\n",
    "       -0.4114   , -0.56649  , -0.024739 , -0.2788   , -0.051034 ,\n",
    "       ...\n",
    "       0.33923  , -0.071309 ,  0.33717  , -0.0037631, -0.23328  ],\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b1eee",
   "metadata": {},
   "source": [
    "Because our instance of `CocoData` has access of all of the captions, we can compute a single embedding vector for each of our captions. Some notes on processing captions:\n",
    " - We will [lowercase, remove punctuation, and tokenize](https://github.com/CogWorksBWSI/LanguageProject_MyGrad/blob/35d2776eb65acb5ef4d713dfc71afb37a5756ec0/image_searcher/text.py#L9-L29) any caption that we work with.\n",
    " - **We will not worry about removing stop (a.k.a glue) words from our captions**.\n",
    "\n",
    "We compute the inverse document frequency (IDF) of every term that appears in the captions, across all captions\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(t)} = \\log_{10}{\\frac{N_{\\mathrm{captions}}}{n_{t}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{t}$ is the number of captions that term-$t$ appears in.\n",
    "\n",
    "Each caption's embedding is created via an IDF-weighted sum of the glove-embedding for each word in the caption.\n",
    "We then normalize this vector\n",
    "E.g, if the caption was \"Horses on a beach\", then the following shape-($D=200$,) embedding would be formed via:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} = \\vec{w}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathrm{norm}(\\vec{w}_{\\mathrm{caption}}) \\rightarrow \\hat{w}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco.compute_caption_embeddings(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43ca48",
   "metadata": {},
   "source": [
    "Calling `coco.compute_caption_embeddings(glove)` will trigger the computation of the IDFs for the words across the captions and then the embedding for each caption.\n",
    "Ultimately `coco` stores a mapping of `caption-ID -> caption-embedding`.\n",
    "The method `coco.caption_id_to_embedding` can be used to retrieve the embedding(s) for one (or more) captions given the caption ID(s).\n",
    "\n",
    "\n",
    "### Loading Image Descriptor Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved features\n",
    "import pickle\n",
    "with Path('data/resnet18_features.pkl').open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab78de",
   "metadata": {},
   "source": [
    "`resnet18_features` is simply a dictionary that stores a $\\vec{d}_{\\mathrm{image}}$ for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image-ID -> shape-(512,) descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fece18",
   "metadata": {},
   "source": [
    "where the image-IDs correspond to those in the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dffb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_keys = sorted(resnet18_features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad5bde",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "\n",
    "The basics of forming our training data is [the following process](https://github.com/CogWorksBWSI/LanguageProject_MyGrad/blob/06d33ee87e035ffd7951807f64657a13d31c76f9/image_searcher/training.py#L78):\n",
    "- **Separate out image IDs into distinct sets for training and validation**\n",
    "- Pick a random training image and one of its associated captions. We'll call these our \"true image\" and \"true caption\"\n",
    "- Pick a different image. We'll call this our \"confusor image\".\n",
    "    - We can also use a fancier way to find a \"harder\" confusor image (THIS IS OPTIONAL)\n",
    "        - For some set \"tournament size\", $n$ (e.g. $n=4$)...\n",
    "        - Pick $n$ random image (must all be distinct from the true image!) and pick an associated caption for each.\n",
    "        - Compare the embeddings of the good caption and the $n$ confusor captions using cosine-similarity.\n",
    "        - The confusor caption with the highest cosine-similarity is the \"hardest\" confusor. We will use the image associated with this caption as our confusor image.\n",
    "\n",
    "Thus our training and each validation data consist of triplets: `(true-caption-ID, true-image-ID, confusor-image-ID)`.\n",
    "We will use batches of these triplets to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from image_searcher.training import generate_data\n",
    "import numpy as np\n",
    "\n",
    "total_images = len(image_keys)\n",
    "train_range = total_images * 4 // 5\n",
    "np.random.shuffle(image_keys)\n",
    "train_image_keys = image_keys[:train_range]\n",
    "valid_image_keys = image_keys[train_range:]\n",
    "\n",
    "# generate train and validation triples\n",
    "triples_train = generate_data(train_image_keys, coco, num_good_captions=30000, tournament_size=1)\n",
    "triples_valid = generate_data(valid_image_keys, coco, num_good_captions=10000, tournament_size=1)\n",
    "\n",
    "len(triples_train), len(triples_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f967ef8",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Our Model\n",
    "\n",
    "Our [model](https://github.com/CogWorksBWSI/LanguageProject_MyGrad/blob/06d33ee87e035ffd7951807f64657a13d31c76f9/image_searcher/model.py#L11) simply consists of one matrix that maps a shape-(512,) image descriptor into a shape-(D=200,) embedded vector, and normalizes that vector.\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} W_{\\mathrm{embed}} = \\begin{bmatrix}\\leftarrow & \\vec{w}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\mathrm{norm}(\\vec{w}_{\\mathrm{image}}) \\rightarrow \\hat{w}_{\\mathrm{image}}\n",
    "\\end{align}\n",
    "\n",
    "<!-- #region -->\n",
    "\n",
    "\n",
    "### Our Loss Function\n",
    "\n",
    "Recall that we have formed triplets of `(true-caption-ID, true-image-ID, confusor-image-ID)`.\n",
    "We will use these to form a triplet of embedding vectors.\n",
    "We can simply look up the embedding vector for our caption:\n",
    " - `true-caption-ID` $\\rightarrow \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}}$\n",
    "\n",
    "And we can retrieve the descriptor vector for both of our images\n",
    "- `true-image-ID` $\\rightarrow \\vec{d}^{\\mathrm{(true)}}_{\\mathrm{image}}$\n",
    "- `confusor-image-ID` $\\rightarrow \\vec{d}^{\\mathrm{(confusor)}}_{\\mathrm{image}}$\n",
    "\n",
    "Processing these descriptors with our model will embed them in the same $D=200$-dimensional space as our captions:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{model}(\\vec{d}^{\\mathrm{(true)}}_{\\mathrm{image}}) &= \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\\\\n",
    "\\mathrm{model}(\\vec{d}^{\\mathrm{(confusor)}}_{\\mathrm{image}}) &= \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We want to embed our image's descriptor in a meaningful way, such that the contents of the image reflect the semantics of its captions.\n",
    "Thus we want\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} > \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\n",
    "We can enforce this using a margin ranking loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{sim}_{\\mathrm{true}} &=  \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} \\\\\n",
    "\\mathrm{sim}_{\\mathrm{confusor}} &=  \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathscr{L}(\\mathrm{sim}_{\\mathrm{true}}, \\mathrm{sim}_{\\mathrm{confusor}}; \\Delta) = \\max(0, \\Delta - (\\mathrm{sim}_{\\mathrm{true}} - \\mathrm{sim}_{\\mathrm{confusor}}))\n",
    "\\end{align}\n",
    "\n",
    "Note that all of our dot-products are involving unit vectors, thus we are computing cosine-similarities.\n",
    "See that this loss function encourages $\\mathrm{sim}_{\\mathrm{true}}$ to be larger than $\\mathrm{sim}_{\\mathrm{confusor}}$ by at least a margin of $\\Delta$.\n",
    "\n",
    "Of course, we will be training on **batches** of triplets. MyGrad's [margin ranking loss](https://mygrad.readthedocs.io/en/latest/generated/mygrad.nnet.losses.margin_ranking_loss.html) will automatically compute the mean over the batch dimension.\n",
    "Note that [einsum](https://mygrad.readthedocs.io/en/latest/generated/mygrad.einsum.html#mygrad.einsum) can be used to take pair-wise dot products across two batches of vectors.\n",
    "E.g. `mg.einsum(\"ni,ni -> n\", a, b)` will take two shape-$(N, D)$ arrays and compute $N$ dot products between corresponding pairs of shape-($D$,) vectors.\n",
    "\n",
    "See [process_batch](https://github.com/CogWorksBWSI/LanguageProject_MyGrad/blob/06d33ee87e035ffd7951807f64657a13d31c76f9/image_searcher/training.py#L152) for implementation details.\n",
    "<!-- #endregion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers import SGD\n",
    "import mygrad as mg\n",
    "from mygrad.nnet.losses import margin_ranking_loss\n",
    "from image_searcher.model import ImageToWord\n",
    "\n",
    "num_epochs = 6\n",
    "batch_size = 32\n",
    "margin = 0.25\n",
    "model = ImageToWord(512, 200)\n",
    "optimizer = SGD(model.parameters, learning_rate=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b13957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_searcher.training import get_image_embeddings, process_batch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(triples_train), batch_size):\n",
    "        loss, acc = process_batch(\n",
    "            triples_train[i : i + batch_size],\n",
    "            model,\n",
    "            margin,\n",
    "            coco=coco,\n",
    "            resnet18_features=resnet18_features,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        plotter.set_train_batch(\n",
    "            dict(loss=loss.item(), accuracy=acc),\n",
    "            batch_size=len(triples_valid[i : i + batch_size]),\n",
    "        )\n",
    "        mg.turn_memory_guarding_off()  # slightly speeds up training\n",
    "    \n",
    "    with mg.no_autodiff:\n",
    "        for i in range(0, len(triples_valid), batch_size):\n",
    "            loss, acc = process_batch(\n",
    "                triples_valid[i : i + batch_size],\n",
    "                model,\n",
    "                margin,\n",
    "                coco=coco,\n",
    "                resnet18_features=resnet18_features,\n",
    "            )\n",
    "            plotter.set_test_batch(\n",
    "                dict(loss=loss.item(), accuracy=acc),\n",
    "                batch_size=len(triples_valid[i : i + batch_size]),\n",
    "            )\n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d3930",
   "metadata": {},
   "source": [
    "## Searching Our Database\n",
    "\n",
    "It is time to create a database of images that we can search through based on user-written queries.\n",
    "We will populate this database **using only images from our validation set** so that we know that the quality of our results isn't from \"overfitting\" on our data.\n",
    "\n",
    "We have trained our embedding matrix, $W_{\\mathrm{embed}}$, we can embed each of the image descriptors from our validation set into the caption semantic space.\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}^{(image)}_1 & \\rightarrow \\\\ \\leftarrow & \\vec{d}^{(image)}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{d}^{(image)}_{N_{val}} & \\rightarrow\\end{bmatrix} \\rightarrow \\mathrm{model(\\dots)} \\rightarrow \\begin{bmatrix}\\leftarrow & \\hat{w}^{(image)}_1 & \\rightarrow \\\\ \\leftarrow & \\hat{w}^{(image)}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\hat{w}^{(image)}_{N_{val}} & \\rightarrow\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This is our \"database\" of images.\n",
    "How do we search for relevant images given a user-supplied query?\n",
    "First, we embed the query in the same way that we embedded the captions (using an IDF-weighted sum of GloVe embeddings).\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{\"horses \\; on \\; a \\; beach\"} \\rightarrow \\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} \\rightarrow \\hat{w}_{\\mathrm{query}}\n",
    "\\end{equation}\n",
    "\n",
    "Then we compute the dot product of this query's embedding against all of our image embeddings in our database.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_1 \\\\ \\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_2 \\\\ \\vdots \\\\ \\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_{N_{val}}\\end{bmatrix} \\rightarrow \\mathrm{top-}k\\;\\mathrm{similarity \\; scores}\n",
    "\\end{align}\n",
    "\n",
    "the top-$k$ cosine-similarities points us to the top-$k$ most relevant images to this query!\n",
    "We need image-IDs associated with these matches and then we can fetch their associated URLs from our `CocoData` instance.\n",
    "The [code for downloading an image is quite simple](https://github.com/CogWorksBWSI/LanguageProject_MyGrad/blob/master/image_searcher/utils.py) thanks to the `PIL` and `requests` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_searcher.search import generate_database, create_search_by_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "database, row2id = generate_database(valid_image_keys, model, resnet18_features, embedding_size=200)\n",
    "searcher = create_search_by_text(database, row2id, coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher(\"horses on a beach\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f10117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"image_embed_model.npy\", \"wb\") as f:\n",
    "    np.save(f, model.dense.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "W = model.dense.weight.data\n",
    "ax.imshow(W.T@W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
